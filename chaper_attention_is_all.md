The short answer is: To prevent the softmax function from saturating, which leads to vanishing gradients and stops the model from learning effectively.Here is the step-by-step breakdown.The Problem: Large Dot ProductsHow are scores calculated? The attention scores are calculated using a dot product between a Query (1$Q$) vector and a Key (2$K$) vector.3What is $d_k$? 4$d_k$ is the dimension of the key and query vectors.5 For example, in the original Transformer paper, 6$d_k$ was 64.7The statistical issue: Let's assume the components of $Q$ and $K$ are independent random variables with a mean of 0 and a variance of 1. A dot product is a sum of $d_k$ products: $Q \cdot K = \sum_{i=1}^{d_k} q_i k_i$.As you add more products together (i.e., as $d_k$ gets larger), the variance of the final dot product increases. Specifically, the variance becomes $d_k$.This means that with a large dimension like $d_k=64$, the dot products can become very large (their standard deviation is $\sqrt{d_k} = \sqrt{64} = 8$).The Consequence: Softmax SaturationThe attention mechanism doesn't use the raw dot product scores. It feeds them into a softmax function to turn them into probabilities (weights that sum to 1).8$$\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}$$The softmax function (which relies on 9$e^x$) is extremely sensitive to large inputs:10Small difference: $\text{softmax}([1, 2, 3])$ -> [0.09, 0.24, 0.67] (The model is "kind of sure" about the 3rd item).Large difference: $\text{softmax}([10, 20, 30])$ -> [2.06e-09, 4.54e-05, 0.9999] (The model is "absolutely certain" about the 3rd item).When the inputs to softmax are very large and spread out (which happens when $d_k$ is large), the softmax "saturates." It pushes all the probability onto a single item, resulting in a distribution that looks like [0, 0, 1, 0].This is terrible for training. When the model is this "certain," the gradient (the signal used to update the model's weights) becomes almost zero. This is the classic vanishing gradient problem. The model becomes overly confident and simply stops learning.The Solution: ScalingTo fix this, we "scale" the dot products before they go into the softmax function:$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$By dividing the scores by 11$\sqrt{d_k}$ (the standard deviation of the dot products), we are effectively normalizing them.12 This keeps the variance of the scores at 1, regardless of how large $d_k$ is.This scaling ensures that:The inputs to the softmax remain in a "reasonable" range.The softmax function doesn't saturate.The gradients remain stable and strong, allowing the model to train efficiently.13In short, dividing by 14$\sqrt{d_k}$ is the "scale" in Scaled Dot-Product Attention, and it's essential for making deep Transformers trainable.15


